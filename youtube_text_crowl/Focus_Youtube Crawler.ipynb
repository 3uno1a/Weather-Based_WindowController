{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube 댓글 Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install webdriver-manager\n",
    "# !pip install selenium\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install pandas\n",
    "# !pip install bs4\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\tech4-28\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.9.2)\n"
     ]
    }
   ],
   "source": [
    " !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def check_comment_count_is_zero(html_source, css_selector):\n",
    "    is_comment_count_zero = False\n",
    "    \n",
    "    soup = BeautifulSoup(html_source, 'lxml')\n",
    "    \n",
    "    datas = soup.select(css_selector)\n",
    "    \n",
    "    if len(datas) > 0:\n",
    "        comment_count_data = datas[0]\n",
    "        \n",
    "        if comment_count_data.text == \"댓글 0개\":\n",
    "            is_comment_count_zero = True\n",
    "            \n",
    "    return is_comment_count_zero\n",
    "\n",
    "\n",
    "def scroll(driver, height=700):\n",
    "    driver.execute_script(f\"window.scrollTo(0, {height});\")\n",
    "\n",
    "\n",
    "\n",
    "def scroll_page(driver):\n",
    "    last_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        \n",
    "        time.sleep(3.0)\n",
    "        \n",
    "        \n",
    "        new_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        \n",
    "        if new_page_height == last_page_height:\n",
    "            break\n",
    "            \n",
    "        last_page_height = new_page_height\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_url_title_in_html_source(html_source, css_selector):\n",
    "    titles, urls = [], []\n",
    "    \n",
    "    soup = BeautifulSoup(html_source, 'lxml')\n",
    "    \n",
    "    datas = soup.select(css_selector)\n",
    "    \n",
    "    for data in datas:\n",
    "        title = data.text.replace('\\n', '')\n",
    "        url = \"https://www.youtube.com\" + data.get('href')\n",
    "        \n",
    "        titles.append(title)\n",
    "        urls.append(url)\n",
    "        \n",
    "    return titles, urls\n",
    "\n",
    "\n",
    "def divide_watch_shorts(titles, urls):\n",
    "    watch_url, shorts_url = [], []\n",
    "    \n",
    "    for title, url in zip(titles, urls):\n",
    "        count = 0\n",
    "        url_type = url.split(\"/\")[3].split(\"?\")[0]\n",
    "        \n",
    "        if url_type == \"watch\":\n",
    "            watch_url.append({\n",
    "                \"title\": title, \n",
    "                \"url\": url\n",
    "            })\n",
    "        elif url_type == \"watch\":\n",
    "            shorts_url.append({\n",
    "                \"title\": title, \n",
    "                \"url\": url\n",
    "            })\n",
    "        print(count + 1 + \"divide_watch_shorts\")\n",
    "    return watch_url, shorts_url\n",
    "        \n",
    "\n",
    "\n",
    "def get_urls_from_youtube_with_keyword(keyword):\n",
    "    \n",
    "    search_keyword_encode = requests.utils.quote(keyword)\n",
    "    \n",
    "    url = \"https://www.youtube.com/results?search_query=\" + search_keyword_encode\n",
    "    \n",
    "    chrome_options = wd.ChromeOptions()\n",
    "    # driver = wd.Chrome(executable_path=\"/Users/jihyepark/Desktop/Window23/youtube/youtube_text_crowl/chromedriver\")\n",
    "    driver = wd.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    driver = scroll_page(driver=driver)\n",
    "        \n",
    "    html_source = driver.page_source\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    css_selector = \"ytd-video-renderer.style-scope.ytd-item-section-renderer > div#dismissible > div.text-wrapper.style-scope.ytd-video-renderer > div#meta > div#title-wrapper > h3.title-and-badge.style-scope.ytd-video-renderer > a#video-title\"\n",
    "    \n",
    "    titles, urls = get_url_title_in_html_source(\n",
    "        html_source=html_source,\n",
    "        css_selector=css_selector\n",
    "    )\n",
    "        \n",
    "    return titles, urls\n",
    "\n",
    "\n",
    "def get_channel_video_url_list(channel_url):\n",
    "    titles = []\n",
    "    urls = []\n",
    "    chrome_options = wd.ChromeOptions()\n",
    "    # driver = wd.Chrome(executable_path=\"/Users/jihyepark/Desktop/Window23/youtube/youtube_text_crowl/chromedriver\")\n",
    "    driver = wd.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    driver.get(channel_url)\n",
    "    \n",
    "    driver = scroll_page(driver=driver)\n",
    "        \n",
    "    html_source = driver.page_source\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    url_title_css_selector = \"ytd-grid-video-renderer.style-scope.ytd-grid-renderer > div#dismissible > div#details > div#meta > h3.style-scope.ytd-grid-video-renderer > a#video-title\"\n",
    "    \n",
    "    titles, urls = get_url_title_in_html_source(\n",
    "        html_source=html_source, \n",
    "        css_selector=url_title_css_selector\n",
    "    )\n",
    "        \n",
    "    return titles, urls\n",
    "\n",
    "def crawl_youtube_page_html_sources(urls):\n",
    "    html_sources = []\n",
    "\n",
    "    for idx in range(len(urls)):\n",
    "        count = 0\n",
    "        chrome_options = wd.ChromeOptions()\n",
    "        # driver = wd.Chrome(executable_path=\"/Users/jihyepark/Desktop/Window23/youtube/youtube_text_crowl/chromedriver\")\n",
    "        driver = wd.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.maximize_window()\n",
    "        driver.get(urls[idx]['url'])\n",
    "        \n",
    "        time.sleep(3.0)\n",
    "        print(count + 1 + \"crawl_youtube_page_html_sources\")\n",
    "        scroll(driver)\n",
    "        \n",
    "        comment_css_selector = \"ytd-comments-header-renderer.style-scope.ytd-item-section-renderer > div#title > h2#count > yt-formatted-string.count-text.style-scope.ytd-comments-header-renderer\"\n",
    "        \n",
    "        WebDriverWait(driver, 100).until(EC.presence_of_element_located((By.CSS_SELECTOR, comment_css_selector)))\n",
    "        \n",
    "        html_source = driver.page_source\n",
    "        \n",
    "        is_comment_count_zero = check_comment_count_is_zero(\n",
    "            html_source=html_source, css_selector=comment_css_selector\n",
    "        )\n",
    "        \n",
    "        if not is_comment_count_zero:\n",
    "            driver = scroll_page(driver=driver)\n",
    "\n",
    "        html_source = driver.page_source\n",
    "        html_sources.append(html_source)\n",
    "\n",
    "        driver.quit()\n",
    "        \n",
    "    return html_sources\n",
    "\n",
    "\n",
    "def post_processing_text(text):\n",
    "    return text.replace('\\n', '').replace('\\t', '').replace('                ','') if text is not None else \"\"\n",
    "\n",
    "\n",
    "def pack_space(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def get_user_IDs_and_comments(url_dict, video_type, html_source):\n",
    "    comment_crawl_result_dict = {\n",
    "        \"title\": url_dict['title'], \n",
    "        \"video_url\": url_dict['url'], \"video_type\": video_type,\n",
    "        \"comment\": []\n",
    "    }\n",
    "    \n",
    "    comment_id_css_selector = \"ytd-comment-renderer#comment > div#body > div#main > div#header > div#header-author > h3.style-scope.ytd-comment-renderer > a#author-text\"\n",
    "    comment_text_css_selector = \"ytd-comment-renderer#comment > div#body > div#main > div#comment-content > ytd-expander#expander > div#content > yt-formatted-string#content-text\"\n",
    "    soup = BeautifulSoup(html_source, 'lxml')\n",
    "\n",
    "\n",
    "    youtube_user_ID_list = soup.select(comment_id_css_selector)\n",
    "    youtube_comment_list = soup.select(comment_text_css_selector)\n",
    "\n",
    "    for youtube_user_id, youtube_comment in zip(youtube_user_ID_list, youtube_comment_list):\n",
    "        user_id = pack_space(text=post_processing_text(text=youtube_user_id.text))\n",
    "        comment = post_processing_text(text=youtube_comment.text)\n",
    "\n",
    "        comment_data_dict = {\"id\":user_id, \"comment\":comment}\n",
    "        \n",
    "        comment_crawl_result_dict['comment'].append(comment_data_dict)\n",
    "    \n",
    "    return comment_crawl_result_dict\n",
    "\n",
    "\n",
    "def convert_crawl_result_dict_to_csv(crawl_result_dict):\n",
    "    title = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…《\\》]', '', crawl_result_dict['title'])\n",
    "    \n",
    "    temp_df = pd.DataFrame(crawl_result_dict['comment'])\n",
    "    \n",
    "    temp_df = temp_df[['id', 'comment']]\n",
    "    \n",
    "    temp_df.to_csv(f\"{title}.csv\", index=False)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 예시 - 키워드 검색 > 동영상 목록 url 크롤링 > 각 영상별 결과 dictionary 생성 > dic to csv 저장\n",
    "### > 현재는 watch url 만 지원 \n",
    "### > 각 결과는 동영상 이름으로 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m crawling_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m titles, urls \u001b[39m=\u001b[39m get_urls_from_youtube_with_keyword(\n\u001b[0;32m      4\u001b[0m     keyword \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m집중할 때 듣는 노래\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      8\u001b[0m watch_url, shorts_url \u001b[39m=\u001b[39m divide_watch_shorts(titles, urls)\n\u001b[0;32m     10\u001b[0m watch_url \u001b[39m=\u001b[39m watch_url[:\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[33], line 101\u001b[0m, in \u001b[0;36mget_urls_from_youtube_with_keyword\u001b[1;34m(keyword)\u001b[0m\n\u001b[0;32m     97\u001b[0m driver\u001b[39m.\u001b[39mquit()\n\u001b[0;32m     99\u001b[0m css_selector \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mytd-video-renderer.style-scope.ytd-item-section-renderer > div#dismissible > div.text-wrapper.style-scope.ytd-video-renderer > div#meta > div#title-wrapper > h3.title-and-badge.style-scope.ytd-video-renderer > a#video-title\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 101\u001b[0m titles, urls \u001b[39m=\u001b[39m get_url_title_in_html_source(\n\u001b[0;32m    102\u001b[0m     html_source\u001b[39m=\u001b[39;49mhtml_source,\n\u001b[0;32m    103\u001b[0m     css_selector\u001b[39m=\u001b[39;49mcss_selector\n\u001b[0;32m    104\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m titles, urls\n",
      "Cell \u001b[1;32mIn[33], line 44\u001b[0m, in \u001b[0;36mget_url_title_in_html_source\u001b[1;34m(html_source, css_selector)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_url_title_in_html_source\u001b[39m(html_source, css_selector):\n\u001b[0;32m     42\u001b[0m     titles, urls \u001b[39m=\u001b[39m [], []\n\u001b[1;32m---> 44\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(html_source, \u001b[39m'\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     46\u001b[0m     datas \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mselect(css_selector)\n\u001b[0;32m     48\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m datas:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[0;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[0;32m    255\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "crawling_result_list = []\n",
    "\n",
    "titles, urls = get_urls_from_youtube_with_keyword(\n",
    "    keyword = \"집중할 때 듣는 노래\"\n",
    ")\n",
    "\n",
    "\n",
    "watch_url, shorts_url = divide_watch_shorts(titles, urls)\n",
    "\n",
    "watch_url = watch_url[:1]\n",
    "\n",
    "# watch_url\n",
    "html_sources = crawl_youtube_page_html_sources(watch_url)\n",
    "\n",
    "for url_dict, html_source in zip(watch_url, html_sources):\n",
    "    crawl_result = get_user_IDs_and_comments(\n",
    "        url_dict=url_dict, \n",
    "        video_type=\"watch\", \n",
    "        html_source=html_source\n",
    "    )\n",
    "    \n",
    "    crawling_result_list.append(crawl_result)\n",
    "\n",
    "\n",
    "for crawl_result in crawling_result_list:\n",
    "    convert_crawl_result_dict_to_csv(\n",
    "        crawl_result_dict=crawl_result\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 예시 - 채널 이동 > 동영상 목록 url 크롤링 > 각 영상별 결과 dictionary 생성 > dic to csv 저장\n",
    "### > 현재는 watch url 만 지원 \n",
    "### > 각 결과는 동영상 이름으로 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawling_result_list = []\n",
    "\n",
    "titles, urls = get_channel_video_url_list(\n",
    "    channel_url=\"https://www.youtube.com/c/%EA%BE%B8%EC%82%90KUPI/videos\"\n",
    ")\n",
    "\n",
    "\n",
    "watch_url, shorts_url = divide_watch_shorts(titles, urls)\n",
    "print(\"=======================1\")\n",
    "watch_url = watch_url[:1]\n",
    "\n",
    "# watch_url\n",
    "html_sources = crawl_youtube_page_html_sources(watch_url)\n",
    "print(\"-----------------------2\")\n",
    "for url_dict, html_source in zip(watch_url, html_sources):\n",
    "    crawl_result = get_user_IDs_and_comments(\n",
    "        url_dict=url_dict, \n",
    "        video_type=\"watch\", \n",
    "        html_source=html_source\n",
    "    )\n",
    "    \n",
    "    crawling_result_list.append(crawl_result)\n",
    "\n",
    "\n",
    "for crawl_result in crawling_result_list:\n",
    "    convert_crawl_result_dict_to_csv(\n",
    "        crawl_result_dict=crawl_result\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
