{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swIQ0b2bUxhe"
      },
      "outputs": [],
      "source": [
        "!pip install librosa\n",
        "!pip install soundfile\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "FitHluuxVgIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "fYb2C8jTUzoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zeroth_data = load_dataset(\"kresnik/zeroth_korean\", 'clean')"
      ],
      "metadata": {
        "id": "_7segjvXV8RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#원본 코드. 첫 데이터셋 생성시 말고는 필요 없는 듯?\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The TensorFlow Datasets Authors and the HuggingFace Datasets Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# Lint as: python3\n",
        "\"\"\"Librispeech automatic speech recognition dataset.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import datasets\n",
        "\n",
        "\n",
        "_CITATION = \"\"\"\\\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "This is Zeroth-Korean corpus,\n",
        "licensed under Attribution 4.0 International (CC BY 4.0)\n",
        "The data set contains transcriebed audio data for Korean. There are 51.6 hours transcribed Korean audio for training data (22,263 utterances, 105 people, 3000 sentences) and 1.2 hours transcribed Korean audio for testing data (457 utterances, 10 people). This corpus also contains pre-trained/designed language model, lexicon and morpheme-based segmenter(morfessor).\n",
        "Zeroth project introduces free Korean speech corpus and aims to make Korean speech recognition more broadly accessible to everyone.\n",
        "This project was developed in collaboration between Lucas Jo(@Atlas Guide Inc.) and Wonkyum Lee(@Gridspace Inc.).\n",
        "\n",
        "Contact: Lucas Jo(lucasjo@goodatlas.com), Wonkyum Lee(wonkyum@gridspace.com)\n",
        "\"\"\"\n",
        "\n",
        "_URL = \"http://www.openslr.org/40\"\n",
        "_DL_URL = \"https://www.openslr.org/resources/40/zeroth_korean.tar.gz\"\n",
        "\n",
        "\n",
        "class ZerothKoreanASRConfig(datasets.BuilderConfig):\n",
        "\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          data_dir: `string`, the path to the folder containing the files in the\n",
        "            downloaded .tar\n",
        "          citation: `string`, citation for the data set\n",
        "          url: `string`, url for information about the data set\n",
        "          **kwargs: keyword arguments forwarded to super.\n",
        "        \"\"\"\n",
        "        super(ZerothKoreanASRConfig, self).__init__(version=datasets.Version(\"1.0.1\", \"\"), **kwargs)\n",
        "\n",
        "\n",
        "class ZerothKoreanASR(datasets.GeneratorBasedBuilder):\n",
        "    \"\"\"Librispeech dataset.\"\"\"\n",
        "\n",
        "    BUILDER_CONFIGS = [\n",
        "        ZerothKoreanASRConfig(name=\"clean\", description=\"'Clean' speech.\")\n",
        "    ]\n",
        "\n",
        "    def _info(self):\n",
        "        return datasets.DatasetInfo(\n",
        "            description=_DESCRIPTION,\n",
        "            features=datasets.Features(\n",
        "                {\n",
        "                    \"file\": datasets.Value(\"string\"),\n",
        "                    \"audio\": datasets.features.Audio(sampling_rate=16_000),\n",
        "                    \"text\": datasets.Value(\"string\"),\n",
        "                    \"speaker_id\": datasets.Value(\"int64\"),\n",
        "                    \"chapter_id\": datasets.Value(\"int64\"),\n",
        "                    \"id\": datasets.Value(\"string\"),\n",
        "                }\n",
        "            ),\n",
        "            supervised_keys=(\"speech\", \"text\"),\n",
        "            homepage=_URL,\n",
        "            citation=_CITATION,\n",
        "        )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        archive_path = dl_manager.download_and_extract(_DL_URL)\n",
        "        #print(archive_path)\n",
        "        return [\n",
        "            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"archive_path\": archive_path, \"split_name\": f\"train_data_01\"}),\n",
        "            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"archive_path\": archive_path, \"split_name\": f\"test_data_01\"}),\n",
        "        ]\n",
        "\n",
        "    def _generate_examples(self, archive_path, split_name):\n",
        "\n",
        "        transcripts_glob = os.path.join(archive_path, split_name, \"*/*/*.txt\")\n",
        "        for transcript_file in glob.glob(transcripts_glob):\n",
        "            path = os.path.dirname(transcript_file)\n",
        "            with open(os.path.join(path, transcript_file), encoding=\"utf-8-sig\") as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    key, transcript = line.split(\" \", 1)\n",
        "                    audio_file = f\"{key}.flac\"\n",
        "                    speaker_id, chapter_id = [int(el) for el in key.split(\"_\")[:2]]\n",
        "                    example = {\n",
        "                        \"id\": key,\n",
        "                        \"speaker_id\": speaker_id,\n",
        "                        \"chapter_id\": chapter_id,\n",
        "                        \"file\": os.path.join(path, audio_file),\n",
        "                        \"audio\": os.path.join(path, audio_file),\n",
        "                        \"text\": transcript,\n",
        "                    }\n",
        "                    yield key, example"
      ],
      "metadata": {
        "id": "LxOj7WHNZtD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_df=pd.read_excel('word_data.xlsx')"
      ],
      "metadata": {
        "id": "t61YDjdfZ_5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "BjzZlzcWwUXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "VXy7vghiwUby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = word_df['단어'].tolist()\n",
        "labels = word_df[['pleasant', 'actived']].values.tolist()"
      ],
      "metadata": {
        "id": "vcn_E0TowUZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "for sentence in sentences:\n",
        "    # 문장을 토큰화하여 토큰 ID와 어텐션 마스크 생성\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])"
      ],
      "metadata": {
        "id": "ZJHkWci7wUeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 ID와 어텐션 마스크를 텐서로 변환\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels, dtype=torch.float)"
      ],
      "metadata": {
        "id": "A6lXlD6Jz1Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터셋 생성\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)"
      ],
      "metadata": {
        "id": "Ju8c9ifHwUga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터로더 생성\n",
        "batch_size = 16\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# BERT 모델 로드\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)"
      ],
      "metadata": {
        "id": "jACkgm13wUin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저 초기화\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# 손실 함수 정의\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "bRWG0puJwUlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 시작\n",
        "num_epochs = 32\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # 배치 데이터를 GPU로 이동\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_masks = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 순전파\n",
        "        outputs = model(input_ids, attention_mask=attention_masks)\n",
        "        predictions = outputs.logits\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 역전파 및 가중치 업데이트\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R-yQh5CwUnO",
        "outputId": "1ffbf5dc-7f6c-4f53-8d46-4305011e1c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32 - Average Loss: 12.9169\n",
            "Epoch 2/32 - Average Loss: 7.4533\n",
            "Epoch 3/32 - Average Loss: 5.5801\n",
            "Epoch 4/32 - Average Loss: 4.8950\n",
            "Epoch 5/32 - Average Loss: 4.5804\n",
            "Epoch 6/32 - Average Loss: 4.3670\n",
            "Epoch 7/32 - Average Loss: 3.7002\n",
            "Epoch 8/32 - Average Loss: 3.0822\n",
            "Epoch 9/32 - Average Loss: 3.1928\n",
            "Epoch 10/32 - Average Loss: 2.7208\n",
            "Epoch 11/32 - Average Loss: 2.2531\n",
            "Epoch 12/32 - Average Loss: 2.0524\n",
            "Epoch 13/32 - Average Loss: 1.8145\n",
            "Epoch 14/32 - Average Loss: 1.7303\n",
            "Epoch 15/32 - Average Loss: 1.5184\n",
            "Epoch 16/32 - Average Loss: 1.5335\n",
            "Epoch 17/32 - Average Loss: 1.1357\n",
            "Epoch 18/32 - Average Loss: 1.0779\n",
            "Epoch 19/32 - Average Loss: 0.7935\n",
            "Epoch 20/32 - Average Loss: 0.7115\n",
            "Epoch 21/32 - Average Loss: 0.6317\n",
            "Epoch 22/32 - Average Loss: 0.5319\n",
            "Epoch 23/32 - Average Loss: 0.5493\n",
            "Epoch 24/32 - Average Loss: 0.4794\n",
            "Epoch 25/32 - Average Loss: 0.4677\n",
            "Epoch 26/32 - Average Loss: 0.4112\n",
            "Epoch 27/32 - Average Loss: 0.3579\n",
            "Epoch 28/32 - Average Loss: 0.2833\n",
            "Epoch 29/32 - Average Loss: 0.2282\n",
            "Epoch 30/32 - Average Loss: 0.1941\n",
            "Epoch 31/32 - Average Loss: 0.1641\n",
            "Epoch 32/32 - Average Loss: 0.1467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 모델 저장\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/emotion_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/emotion_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvJtgnHMwUpn",
        "outputId": "01d0cc0e-ad62-43df-ce63-012769cf5cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/emotion_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/emotion_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/emotion_model/vocab.txt',\n",
              " '/content/drive/MyDrive/Colab Notebooks/emotion_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기까지 감정 단어 모델"
      ],
      "metadata": {
        "id": "ZCWl4AxJe62o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zeroth_sentence = zeroth_data['test']['text']"
      ],
      "metadata": {
        "id": "_XAvPC3DeNZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델과 토크나이저 불러오기\n",
        "model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/emotion_model\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/emotion_model\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "1HKOKwZ2krVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_predictions(predictions):\n",
        "    emotion_labels = ['pleasant', 'actived']\n",
        "    scores = predictions.tolist()\n",
        "    results = []\n",
        "\n",
        "    for score in scores:\n",
        "        result = {}\n",
        "        for i, emotion_label in enumerate(emotion_labels):\n",
        "            result[emotion_label] = score[i]\n",
        "        results.append(result)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "iRuAqK6Xk-cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sentence in zeroth_sentence :\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids.append(encoded['input_ids'])\n",
        "    attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)"
      ],
      "metadata": {
        "id": "wxndvKYKeNjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_masks = attention_masks.to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_masks)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "\n",
        "scores = analyze_predictions(predictions)"
      ],
      "metadata": {
        "id": "ePHExa-BeNoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#결과물 저장\n",
        "def save_prediction_results(sentences, scores, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"Text\\tPleasant\\tActived\\n\")\n",
        "        for sentence, score in zip(sentences, scores):\n",
        "            f.write(f\"{sentence}\\t{score['pleasant']}\\t{score['actived']}\\n\")\n",
        "output_file = 'prediction_results.txt'\n",
        "save_prediction_results(zeroth_sentence, scores, output_file)\n",
        "print(f\"Prediction results saved to {output_file}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8jHDACFjnXr",
        "outputId": "762dbec2-a7d3-4ca0-8fca-c4d10fc90989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction results saved to prediction_results.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "/content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "id": "uwq4fykh7elW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "Nm6mWKBao7TI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkeAVk6jt9F9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}